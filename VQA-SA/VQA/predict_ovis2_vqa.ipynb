{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "device = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(r\"/data2/pengziyang/huggingface/Ovis2-34B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             multimodal_max_length=32768,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=device)\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cot_suffix = \"Provide a step-by-step solution to every problems, and conclude with 'the answer is' followed by the final solution one by one.\"\n",
    "# cot_suffix = \"Provide a step-by-step solution to the problem, and conclude with 'the answer is' followed by the final solution.\"\n",
    "cot_suffix = \"请使用中文进行回答；对属于人和车的目标必须先分析他们的朝向，同时考虑目标在图中的位置与其到镜头的距离，提供解决问题的详细步骤，并在回答的末尾用‘答案是...’给出最终答案。\"\n",
    "# cot_suffix = \"请使用中文进行回答；对属于人和车的目标必须先分析他们的朝向，同时考虑目标在图中的位置与其到镜头的距离，提供解决问题的详细步骤。\"\n",
    "# cot_suffix = \"Please provide the bounding box coordinate range from of the region this sentence describes.\"\n",
    "# image_path = r\"/data2/pengziyang/data/VQA_SA/images/22_2_111_0000018_250205.jpg\"\n",
    "# image_path = r\"./sln/VQA_SA/images/19_4_101_0000034_240516.jpg\"\n",
    "image_path = r\"./sln/VQA_SA/images/12_0_101_0000020_250222.jpg\"\n",
    "# image_path = r\"./sln/VQA_SA/images/26_4_104_0000036_250219.jpg\"\n",
    "images = [Image.open(image_path)]\n",
    "# images = []\n",
    "max_partition = 9\n",
    "# history = \"\"\"\n",
    "# Here are some questions you answered before:\n",
    "\n",
    "# Excuse me, which of the trash can in the picture and the child in white is closer to the photographer? Please choose from the trash can and the child in white\n",
    "# Output:\n",
    "# To determine which is closer to the photographer, the trash can or the child in white, we need to analyze their positions in the image. The child in white is walking on the pathway, while the trash can is located to the right side of the image. By comparing their relative distances, it is clear that the child in white is closer to the photographer than the trash can.\n",
    "\n",
    "# Thus, the answer is the child in white.\n",
    "\n",
    "# From the perspective of the little girl in white, in which direction is the trash can located? Please answer from the left, right, front, back, left front, right front, left back, and right back\n",
    "\n",
    "# Output:\n",
    "# To determine the direction of the trash can from the perspective of the little girl in white, we first locate the girl in the image. She is walking on the pathway, holding hands with an adult. Next, we identify the trash can, which is to the right side of the image. From the girl's perspective, the trash can is to her right. Therefore, the trash can is located to her right front.\n",
    "\n",
    "# Thus, the answer is right front.\n",
    "\n",
    "# \"\"\"\n",
    "# text = \"\"\"他们中的哪一个更高？请回答水杯或者垃圾桶\"\"\"\n",
    "# text = \"\"\"从摄像角度看，路障在黑色电动车的哪个方向？请从前方、后方、左方、右方、上方、下方里选择一个方向\n",
    "\n",
    "# 路障 位于 [731, 965, 780, 1264] 深度为 3.8\n",
    "# 黑色电动车 位于 [839, 954, 1086, 1283] 深度为 3.8\n",
    "\n",
    "# 已经有两个回答：\n",
    "# 1、要从摄像头的视角确定路障相对于黑色电动车的方向，首先需要在图像中定位路障和黑色电动车。黑色电动车在远处可见，路障位于汽车的右侧。由于摄像头正对着场景，路障的位置应在黑色电动车的右侧。\\n\\n因此，答案是正确的。\n",
    "# 2、首先，观察图像中路障和黑色电动车的位置。路障位于图像的右侧，而黑色电动车位于路障的右侧。根据深度信息，路障和黑色电动车的深度值均为3.8，说明它们距离镜头的距离相同。因此，从摄像机的视角来看，路障位于黑色电动车的左方。\\n\\n答案是左方。\n",
    "\n",
    "# 请根据图像内容，从两种回答中整理出新的正确回答，请务必确保最后输出的答案在问题给出的答案选项中。\n",
    "# \"\"\"\n",
    "# text = \"\"\"汽车在戴头盔骑电动车的人的哪个方向？请从前方、后方、左方、右方、上方、下方、左前方、右前方、左后方、右后方中选择\n",
    "\n",
    "# 汽车 位于 [563, 964, 710, 1071] 深度为 17.8\n",
    "# 戴头盔骑电动车的人 位于 [703, 964, 801, 1125] 深度为 10.5\n",
    "\n",
    "# 已经有两个回答：\n",
    "# 1、若要确定汽车相对于戴头盔骑电动自行车的人的方向，首先需要在图像中定位这两个人。电动自行车骑行者位于图像的右侧，而汽车则位于他们的前方，稍偏左。通过分析他们的位置，我们可以得出结论：相对于骑电动自行车的人来说，汽车位于左前方。\\n\\n因此，正确答案是左前方。\n",
    "# 2、为了确定汽车相对于戴头盔骑电动车的人的位置，我们首先分析图像中两个目标的相对位置和方向。汽车位于图像的中间偏左位置，而戴头盔骑电动车的人位于图像的右下方。根据深度信息，汽车的深度值为17.8，而戴头盔骑电动车的人的深度值为10.5，说明汽车更远离镜头。结合图像中两者的相对位置，汽车位于戴头盔骑电动车的人的左后方。\\n\\n因此，答案是左后方。\n",
    "\n",
    "# 请根据图像内容，从两种回答中整理出新的正确回答，请务必确保最后输出的答案在问题给出的答案选项中。\n",
    "# \"\"\"\n",
    "text = \"\"\"图中黄色标识牌在撑伞的人的哪个方向？请从前方、后方、左方、右方、上方、下方、左前方、右前方、左后方、右后方中选择\n",
    "\n",
    "黄色标识牌 位于 [416, 365, 440, 437] 深度为 15.7\n",
    "撑伞的人 位于 [1260, 543, 1417, 908] 深度为 3.1 \n",
    "撑伞的人朝向镜头方向的正左方\n",
    "\n",
    "已经有两个回答：\n",
    "1、要确定拿伞的人所指的黄色指示牌的方向，我们需要分析图像。拿伞的人位于图像的右侧。黄色指示牌位于拿伞的人的上方和左侧。因此，黄色指示牌指向拿伞的人的左前方。\\n\\n所以，正确答案是左前方。\n",
    "2、首先，观察图像中撑伞的人的位置，位于图像的右侧。黄色标识牌位于图像的左侧，靠近道路的上方。根据图像中的位置和深度信息，黄色标识牌在撑伞的人的左后方。因此，黄色标识牌相对于撑伞的人的位置是左后方。\\n\\n答案是左后方。\n",
    "\n",
    "请根据图像内容，从两种回答中整理出新的正确回答，请务必确保最后输出的答案在问题给出的答案选项中。\n",
    "\"\"\"\n",
    "# text = \"\"\"以图中人的视角来看，请问铲子在他的哪个方位？请从前面或后面中选择一个。\n",
    "# 图中人[430, 533, 717, 1049](深度 1.4)\n",
    "# 铲子[163, 1175, 284, 1249](深度 0.3)\"\"\"\n",
    "# text = \"\"\"图中人 位于 [430, 533, 717, 1049] 深度 1.4 面向镜头视角的正左方\n",
    "# 铲子 位于[163, 1175, 284, 1249] 深度 0.3\n",
    "# 以图中人的视角来看，请问铲子在他的哪个方位？请从前面、后面、左侧、右侧中选择一个。\"\"\"\n",
    "# text = \"\"\"The red and white bucket on the far right of the center of the road near the junction\"\"\"\n",
    "# query = f'{history}<image>\\n{text}\\n{cot_suffix}'\n",
    "query = f'<image>\\n{cot_suffix}: {text}'\n",
    "# query = f'{cot_suffix}: {text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, input_ids, pixel_values = model.preprocess_inputs(query, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        top_k=None,\n",
    "        temperature=None,\n",
    "        repetition_penalty=None,\n",
    "        eos_token_id=model.generation_config.eos_token_id,\n",
    "        pad_token_id=text_tokenizer.pad_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = text_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(f'Output:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2 as cv\n",
    "from Portable import ipy\n",
    "# import re\n",
    "\n",
    "# perc_pattern = r\"\\[(\\d\\.\\d+)[^\\d\\.]+(\\d\\.\\d+)[^\\d\\.]+(\\d\\.\\d+)[^\\d\\.]+(\\d\\.\\d+)\\]\"\n",
    "# coords = [float(e) for e in re.findall(perc_pattern, output)[0]]\n",
    "\n",
    "# canvas = cv.imread(image_path, cv.IMREAD_COLOR)\n",
    "# H, W, C = canvas.shape\n",
    "# coords = [round(e * W if i % 2 == 0 else e * H) for i, e in enumerate(coords)]\n",
    "# canvas = cv.rectangle(canvas, coords[:2], coords[2:], (0, 0, 255), 2)\n",
    "# print(text)\n",
    "# ipy.display_np_images(canvas, resize=0.4, bgr2rgb=True)\n",
    "\n",
    "ipy.display_image_files(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
