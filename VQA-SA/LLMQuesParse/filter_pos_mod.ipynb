{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤掉问题中的位置修饰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengzy/mambaforge/envs/qwen2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-26 16:51:54,680] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengzy/mambaforge/envs/qwen2/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/home/pengzy/mambaforge/envs/qwen2/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import utils.message_constructor as mc\n",
    "import utils.vlm_utils as vu\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"./sln/VG_RS/VG-RS-question.json\"\n",
    "llm_base = r\"./sln/qwen2.5_lm_7B\"\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, \"r\", encoding=\"utf8\") as reader:\n",
    "    data = json.load(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlm based on <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> is loaded\n"
     ]
    }
   ],
   "source": [
    "vu.ensure_vl_base(\n",
    "    base_path=llm_base,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = vu.vl_model\n",
    "llm_processor = vu.vl_processor\n",
    "\n",
    "def qwen25_infer(\n",
    "        messages:mc.Message,\n",
    "        max_new_tokens:int=vu.DEFAULT_MAX_NEW_TOKENS\n",
    "    ) -> str:\n",
    "    text = llm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = llm_processor(text=[text], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(llm_model.device)\n",
    "    generated_ids = llm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = llm_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return \"\".join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "15869\n",
      "{'image_path': 'images\\\\27_3_108_0000002_250305.jpg', 'question': 'Wall-mounted vanity with a white countertop'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.prompts as sp\n",
    "import random as rand\n",
    "\n",
    "chosen_idx = rand.randint(0, len(data) - 1)\n",
    "chosen = data[chosen_idx]\n",
    "\n",
    "print(chosen_idx)\n",
    "print(chosen)\n",
    "\n",
    "message = mc.Message().add_role_block(\"system\", sp.FILTER_POSMOD_SYSTEM)\\\n",
    "                .add_role_block(\"user\", chosen[\"question\"])\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wall-mounted vanity with a white countertop\n"
     ]
    }
   ],
   "source": [
    "output = qwen25_infer(message, 2048)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
